<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="The last thing I&rsquo;ll cover in this series for the time being is storage. This
is perhaps the most crucial part of a complete homelab setup, as without
storage you are very limited in what you can do that is useful. Essentially I
wanted expandable, reliable, and moderately fast storage that could be accessed
by all the nodes in my Talos cluster. Turns out I had to give up a bit of high
availability to get there, but that&rsquo;s okay."><meta http-equiv=Content-Security-Policy content="default-src 'self' cdn.jsdelivr.net; style-src 'self' 'unsafe-inline'; img-src *; object-src 'none'; script-src 'self' 'unsafe-inline' cdn.jsdelivr.net; worker-src 'self' blob:; base-uri 'self'"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><title>Talos Linux Homelab Pt. 3: Storage Solutions</title><link rel=preload as=font type=font/woff2 href=/fonts/source-sans-pro-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/source-sans-pro-600.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/inconsolata-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/inconsolata-600.woff2 crossorigin><style type=text/css>@font-face{font-display:swap;font-family:source sans pro;font-style:normal;font-weight:400;src:url(/fonts/source-sans-pro-regular.woff2)format("woff2"),url(/fonts/source-sans-pro-regular.woff)format("woff")}@font-face{font-display:swap;font-family:source sans pro;font-style:normal;font-weight:600;src:url(/fonts/source-sans-pro-600.woff2)format("woff2"),url(/fonts/source-sans-pro-600.woff)format("woff")}@font-face{font-display:swap;font-family:inconsolata;font-style:normal;font-weight:400;src:url(/fonts/inconsolata-regular.woff2)format("woff2"),url(/fonts/inconsolata-regular.woff)format("woff")}@font-face{font-display:swap;font-family:inconsolata;font-style:normal;font-weight:600;src:url(/fonts/inconsolata-600.woff2)format("woff2"),url(/fonts/inconsolata-600.woff)format("woff")}footer#page-footer,nav#site-nav{background:0 0;padding:1em}footer#page-footer,nav#site-nav{background:#282a2e;color:#c5c8c6;line-height:3em}nav#site-nav{display:flex;overflow:auto;scrollbar-width:none}nav#site-nav::-webkit-scrollbar{display:none}nav#site-nav a{display:inline-block;width:6em;font-weight:700;text-align:center;color:inherit;flex-shrink:0}@media screen and (min-width:60em){footer#page-footer,nav#site-nav{line-height:inherit}}html{background:#252628}body{width:100%;max-width:60em;margin:0 auto;line-height:1.5em;font-family:source sans pro,helvetica,liberation sans,open sans,sans-serif;font-size:100%;color:#c5c8c6}@media screen and (min-width:60em){body{margin:2em auto!important}}h1,h2,h3,h4,h5,h6{margin:1.1em 0 .83em}h1,h2,h3{color:#8c9440}h4,h5,h6{color:#b5bd68}h1{font-size:1.5em}h2{font-size:1.26em}h3{font-size:1.1em}h4{font-size:1.03em}h5{font-size:.67em}h6{font-size:.51em}a{text-decoration:initial;color:#de935f}a:hover{text-decoration:underline}em{color:#f0c674;font-style:normal}strong{color:#c66}nav#site-nav{border-radius:1em 1em 0 0;background:0 0;color:#c5c8c6}nav#site-nav a.push-right{margin-left:auto}nav#site-nav a.current{color:#81a2be}label#toc-toggle{display:flex;width:100%;margin-top:1em;height:3em;align-items:center;justify-content:center}input#toc-toggle-box{display:none}input#toc-toggle-box~nav#TableOfContents{display:none}input#toc-toggle-box:checked~nav#TableOfContents{display:block}nav#TableOfContents{margin:.5em 0 0;box-sizing:border-box;background:#282a2e;padding:1em;border-radius:.5em;overflow:hidden;font-weight:700;color:#de935f}nav#TableOfContents ul{margin:0;padding:0;list-style-type:none}nav#TableOfContents ul li{overflow:hidden;white-space:nowrap;text-overflow:' ...'}nav#TableOfContents ul li+li{margin-top:1em}nav#TableOfContents ul ul{padding-left:1em;margin-top:1em}@media screen and (min-width:60em){nav#TableOfContents{float:right;margin:1em 0 1em 1em;max-width:40%;font-size:85%}nav#TableOfContents ul li+li{margin-top:initial}nav#TableOfContents ul ul{margin-top:initial}}@media screen and (min-width:60em){label#toc-toggle{display:none}}@media screen and (min-width:60em){input#toc-toggle-box~nav#TableOfContents{display:initial}}footer#page-footer{background:0 0;color:#707880;display:grid;grid-auto-flow:column;grid-template-columns:minmax(0,1fr)}footer#page-footer a{color:#f0c674}footer#page-footer span.commit-subject{color:#de935f;margin-left:.5em}footer#page-footer p#commit{white-space:nowrap;text-overflow:ellipsis;overflow:hidden}footer#page-footer p#commit span.commit-hash{font-family:inconsolata,dejavu sans mono,liberation mono,monospace}footer#page-footer p#timestamp,footer#page-footer p#commit{margin:0}footer#page-footer p#timestamp{text-align:right}article{background:#1d1f21;padding:.1em 1.5em .8em}@media screen and (min-width:60em){article{border-radius:.5em}}article header{font-weight:700}article header section.header-line{display:flex;justify-content:space-between;align-items:center}article header section.header-line h1{color:#8c9440;margin-bottom:.25em}article header section.header-line aside#translations{margin:1.1em 0 .25em}article header section.header-line aside#translations ul#translations{list-style:none;padding:0;margin:0;display:flex;justify-content:space-around;font-size:115%}article header section.header-line aside#translations ul#translations li:not(:first-child){margin-left:1em}article header section.subheader-line{display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap}article header section.subheader-line p.subtitle{color:#5e8d87;margin:0}article header section.subheader-line p.subtitle em{color:inherit;font-style:italic}article header section.subheader-line ul#article-tags{list-style-type:none;padding-left:0;margin:0;font-size:85%}article header section.subheader-line ul#article-tags li{display:inline-block}article header section.subheader-line ul#article-tags li:nth-child(n+2){margin-left:1em}article header section.subheader-line ul#article-tags li a{color:#f0c674}article blockquote{color:#979a98;padding-left:1em;border:0 solid #707880;border-left-width:.5em}article table{width:100%}article table th{color:#81a2be}article table td{color:#c5c8c6}article table td.date{width:1%;white-space:nowrap;padding:0 .5em 0 0}article table.compare-table{border-collapse:collapse}article table.compare-table th,article table.compare-table td{padding:.5em 1em;box-sizing:border-box}article table.compare-table th:first-child,article table.compare-table td:first-child{padding-left:0}article table.compare-table th:last-child,article table.compare-table td:last-child{padding-right:0}article table.compare-table th,article table.compare-table td:nth-child(n+2){text-align:center}article table.compare-table td{border-top:.1em solid #707880}article table.compare-table th:nth-child(n+2){min-width:4em}article li::marker{color:#f0c674}article figure{margin:1.25em 0;text-align:center}article figure img{min-height:5em;max-height:15em;width:100%;object-fit:cover;border-radius:1em}article figure figcaption p{margin-top:0}article code{font-family:inconsolata,dejavu sans mono,liberation mono,monospace;font-size:100%}article pre{background:#282a2e;border-radius:.5em;padding:.5em 1em;overflow-x:auto;line-height:initial}article>hr{color:#707880;opacity:.5;margin:2em 0}article p code,article li code{background:#282a2e;padding:0 .3em;border-radius:.25em}article>p a,article>ul li a,article>ol li a{text-decoration:underline}article.extra-pages h2~p{margin-left:2em}article section.dialog{display:flex;padding:1em;align-content:center;border-radius:.5em;border:.2em solid #707880;margin:.5em 0 0;box-sizing:border-box;max-width:100%}article section.dialog p:first-child{margin-top:0}article section.dialog p:last-child{margin-bottom:0}article section.dialog .dialog-icon{margin-right:1em}article section.dialog.dialog-info{border-color:#5f819d}article section.dialog.dialog-warning{border-color:#de935f}article section.dialog.dialog-error{border-color:#a54242}article section.dialog.dialog-success{border-color:#8c9440}@media screen and (min-width:60em){article section.dialog{margin:1em 0 0}}</style></head><body><nav id=site-nav><a href=/ class=[]>Home
</a><a href=/blog/ class=[]>Blog
</a><a href=/glob/ class=[]>Glob
</a><a href=/projects/ class=[]>Projects
</a><a href=/about/ class=push-right>About</a></nav><main role=main><article id=content><header><section class=header-line><h1>Talos Linux Homelab Pt. 3: Storage Solutions</h1></section><section class=subheader-line><p class=subtitle>Can&rsquo;t do much if you can&rsquo;t remember anything</p><ul id=article-tags><li><a href=/tags/devops>DevOps</a></li><li><a href=/tags/homelab>Homelab</a></li><li><a href=/tags/kubernetes>Kubernetes</a></li><li><a href=/tags/talos>Talos</a></li><li><a href=/tags/technology>Technology</a></li></ul></section></header><input type=checkbox id=toc-toggle-box>
<label for=toc-toggle-box id=toc-toggle><em>• Table of Contents •</em></label><nav id=TableOfContents><ul><li><a href=#the-low-hanging-fruit-longhorn>The low-hanging fruit: Longhorn</a></li><li><a href=#building-a-truenas-server>Building a TrueNAS server</a><ul><li><a href=#raidz1-hdd-array>RAIDz1 HDD array</a><ul><li><a href=#probability-of-failure>Probability of failure</a></li><li><a href=#datasets-within-it>Datasets within it</a></li></ul></li><li><a href=#the-spare-nvme-ssd>The spare NVMe SSD</a></li></ul></li><li><a href=#storage-rules-of-thumb>Storage rules of thumb</a></li></ul></nav><p>The last thing I&rsquo;ll cover in this series <em>for the time being</em> is storage. This
is perhaps the most crucial part of a complete homelab setup, as without
storage you are very limited in what you can do that is useful. Essentially I
wanted expandable, reliable, and moderately fast storage that could be accessed
by all the nodes in my Talos cluster. Turns out I had to give up a bit of high
availability to get there, but that&rsquo;s okay.</p><p>The final setup resulted in the following storage classes:</p><ul><li><code>longhorn</code>: highly available block storage spread across cluster nodes, able
to schedule replicas on the same node as workloads.</li><li><code>truenas-iscsi-nvme</code>: block storage served from a TrueNAS server over iSCSI,
housed on a spare 2 TB NVMe drive.</li><li><code>truenas-iscsi-hdd</code>: same as above, but housed on a RAIDz1 array of 4x 8 TB
hard drives that cost me more than I wish they had.</li><li><code>truenas-nfs-hdd</code>: NFS volumes served from the same RAIDz1 array of 4x 8 TB
hard drives, for data that needs to be shared between multiple services or
accessible from my desktop.</li></ul><p>I also ended up building a new desktop on which to install TrueNAS. Expensive
stuff.</p><h2 id=the-low-hanging-fruit-longhorn>The low-hanging fruit: Longhorn</h2><p><a href=https://longhorn.io/>Longhorn</a> is a CNCF project that provides highly
available <em>node-backed</em> block storage for Kubernetes clusters. It lets you make
use of local disks on your cluster nodes to provide persistent storage for your
workloads. It&rsquo;s easy to set up, but since Talos is an immutable OS, you have to
create an <code>rw</code> bind mount for it to use:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># talos/patches/longhorn-mount.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>machine</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>kubelet</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>extraMounts</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>destination</span>: <span style=color:#ae81ff>/var/lib/longhorn</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>type</span>: <span style=color:#ae81ff>bind</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>source</span>: <span style=color:#ae81ff>/var/lib/longhorn</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>options</span>: [ <span style=color:#ae81ff>bind, rshared, rw ]</span>
</span></span></code></pre></div><p>A standard installation of Longhorn creates a storage class that defaults to 3
replicas for each volume you create. Each of these replicas is stored on a
different cluster node, and if one node goes down, your data is still safe and
available for when your containers get scheduled elsewhere.</p><section class="dialog dialog-warning"><section class=dialog-icon>⚠️</section><section class=dialog-content><p>When upgrading Longhorn to a new version, make sure to follow upgrade
instructions from its documentation. It is not as simple as updating to the
newest version of the official Helm chart. <strong>Not doing so may result in data
loss.</strong></p></section></section><h2 id=building-a-truenas-server>Building a TrueNAS server</h2><p>For the rest of my storage needs, I needed a dedicated storage server. I&rsquo;d
never had a proper NAS before, only a 4 TB external hard drive plugged into an
old laptop serving simultaneously as an HTPC and home server years ago. So this
was going to be new territory for me.</p><p>To get started, I built a new desktop PC to run TrueNAS on. It needed to be
relatively affordable, quiet, stable, and have a case designed to fit multiple
hard drives. The case I settled on was the <em>Fractal Design Node 804,</em> and for
the sake of upgradability, I went with an AM5 motherboard and CPU. If you&rsquo;re
curious, <a href=https://ca.pcpartpicker.com/list/zZCwxg>this is the full parts list</a> on PCPartPicker.</p><p><em>What do I call it?</em> All my homelab nodes are named after provincial capitals
in Canada, but since this lies outside the cluster itself, I decided to name it
<em>Windsor</em>, after the city that borders Detroit. It&rsquo;s a major trade hub with the
United States.</p><figure><a href=/media/home-infrastructure-diagram.webp><img src=/media/home-infrastructure-diagram.webp alt="Diagram of the infrastructure here at home."></a><figcaption><p>Diagram of the infrastructure here at home.</p></figcaption></figure><p>Installing TrueNAS is a relatively straightforward endeavor. <em>My chronic bad
luck, however, struck again,</em> <strong>twice:</strong></p><ul><li>Ventoy couldn&rsquo;t boot TrueNAS SCALE&rsquo;s ISO image. <a href=https://github.com/ventoy/Ventoy/issues/3069>#3069</a></li><li>ACME registration was broken on TrueNAS due to a removed field.
<a href=https://github.com/truenas/middleware/pull/16646>#16646</a></li></ul><p>It just couldn&rsquo;t be easy, huh?</p><h3 id=raidz1-hdd-array>RAIDz1 HDD array</h3><p>Hard drives are expensive, and sacrificing the price of one for the sake of
redundancy was a bitter pill to swallow. The readers of this website &mdash;
assuming they exist &mdash; are probably aware of what RAID arrays are, and what
RAIDz1 means. That&rsquo;s all this section covers, really, so I&rsquo;ll be brief.</p><p>RAIDz1 is ZFS&rsquo; implementation of RAID 5. It stripes data across multiple
drives, with parity information distributed among them. This means <em>if one
drive fails, no data is lost.</em> Unless, that is, you don&rsquo;t rebuild the array
before a second drive fails. Note that <strong>the more drives are in the array, the
higher the odds of failure.</strong></p><h4 id=probability-of-failure>Probability of failure</h4><p>Let \(p \in (0,1)\) be the probability of a single drive failing during a given
time period. The probability of at least one drive failing in an array of \(n
\in \mathbbm{N}\setminus \{0,1\}\) drives derives from \(n\) as follows:</p><ol><li>The variable \(F\) models the failure of a single drive as a Bernoulli
random variable with parameter \(p\): \(F \sim Bern(p)\).</li><li>Though the probability of each drive failing is different, assume \(p\) to
be the worst case scenario across all drives.</li><li>Assume drive failures are independent events.</li><li>The variable \(C\) models \(n\) independent events of \(F\), meaning \(C
\sim Bin(n, p)\).</li></ol><p>From <em>4</em>, the following is the probability of no drives failing in a given
period:</p>$$
\mathbbm{P}(C = 0) = \mathbbm{P}(F = 0)^n = (1 - p)^n
$$<p>Finally, the variable \(A\) models the event of at least one drive failing
(\(C \geq 1\)) after \(t\) hours of operation. It has a geometric
distribution:</p>$$
\begin{aligned}
\mathbbm{P}(A = t) &= (\mathbbm{P}(C = 0))^{t-1} \cdot \mathbbm{P}(C \geq 1) \\
&= (1 - p)^{n(t-1)} \cdot \left(1 - (1 - p)^n\right) \\
&= (1 - p)^{n(t-1)} - (1 - p)^{nt}
\end{aligned}
$$<p>Because \(A \sim Geom(q)\) with \(q = 1 - (1 - p)^n\), we know as time
progresses, the probability of at least one drive failing approaches 1.
However, what we want to demonstrate is that as \(n\) increases, so does \(q\),
meaning <em>the probability of at least one drive failing in a given time period
increases with the number of drives in the array.</em> This is intuitive:</p>$$
1 - p \in (0,1) \implies (1 - p)^n > (1 - p)^{n+1} \implies 1 - (1 - p)^n < 1 -
(1 - p)^{n+1}
$$<p>Thus, the formalism shows more drives leads to higher risk of failure. That
concludes this entirely unnecessary aside.</p><h4 id=datasets-within-it>Datasets within it</h4><p>My 4 HDDs are in such an array, giving me a total of 24 TB of usable space.
This RAIDz1 array comprises the <code>wolves</code> dataset in TrueNAS. Inside of it, I
created a <code>k8s</code> dataset to hold all my cluster&rsquo;s volumes. Within it, two
datasets:</p><ul><li><code>iscsi</code>, for iSCSI volumes, which won&rsquo;t be shared between multiple replicas.</li><li><code>nfs</code>, for NFS shares, which can be mounted by multiple replicas and by my
desktop.</li></ul><p>In each of these, there are also two datasets:</p><ul><li><code>volumes</code>, which holds the volumes themselves, and</li><li><code>snapshots</code>, which holds periodic snapshots of the volumes for backup
purposes.</li></ul><p>This is particularly useful for the <a href=https://github.com/democratic-csi/democratic-csi>democratic-csi</a> setup.</p><h3 id=the-spare-nvme-ssd>The spare NVMe SSD</h3><p>I had an extra 2 TB NVMe SSD lying around and decided to put it to use for
storage that doesn&rsquo;t need redundancy. I could claim it benefits from speed, but
the 1 Gbps network limit makes that a moot point. The only arguments in favor
of using this drive are potentially lower latency and not taking up space in my
RAIDz1 array for things I can afford to lose.</p><p>This is the only block storage device in the <code>nvme</code> dataset, in which the <code>k8s</code>
dataset structure was reproduced.</p><h2 id=storage-rules-of-thumb>Storage rules of thumb</h2><p>In the end, the following rules dictate how I choose a storage class for any
given workload&rsquo;s persistent storage needs:</p><ol><li><em>Does it need to be fast?</em> Then use <code>longhorn</code>, as it can achieve data
locality on the node where the workload is scheduled.</li><li><em>Can we afford to lose data in the event of disaster?</em> If so, use any of the
<code>-nvme</code> classes, but otherwise use <code>-hdd</code>.</li><li><em>Does it need to be shared between multiple replicas or services?</em> Then use
NFS, but otherwise use iSCSI.</li></ol><p>Number 1 is the most important rule, as databases and similar workloads benefit
greatly from low latency and high throughput. The RAID array has been used for
media libraries and backups, while the NVMe drive has been used only for
testing purposes so far.</p></article></main><footer id=page-footer><p id=commit><a href=https://github.com/d3adb5/website/commit/4a03acf1293cfbf65f5f934686dc3c6e8772b4db><span class=commit-hash>4a03acf</span><span class=commit-subject>content(blog): add part 3 of talos homelab series</span></a></p><p id=timestamp>Updated October 26, 2025</p></footer></body><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]],packages:{"[+]":["bbm"]}},loader:{load:["ui/safe","[tex]/bbm"]}}</script></html>